{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "wanted_pre_onboarding.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPM1ZNT6Owb20FCpJ/q/Ayv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/allusinc/wanted_pre_onboarding/blob/main/wanted_pre_onboarding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "-vfTXd9drgZ9"
      },
      "outputs": [],
      "source": [
        "class Tokenizer():\n",
        "    def __init__(self):\n",
        "        self.word_dict = {'oov': 0}\n",
        "        self.fit_checker = False\n",
        "        \n",
        "    def preprocessing(self, sequences):\n",
        "        result = []\n",
        "        for i in sequences:\n",
        "            sentence = []\n",
        "            for j in i.split(' '):\n",
        "                if j != ' ':\n",
        "                    sentence.append(''.join(filter(str.isalnum, j.lower())))\n",
        "            result.append(st)\n",
        "        return result\n",
        "    \n",
        "    def fit(self, sequences):\n",
        "        self.fit_checker = False\n",
        "        tokens = self.preprocessing(sequences)\n",
        "        value = max(self.word_dict.values()) + 1\n",
        "        for multiple_token in tokens:\n",
        "            for token in multiple_token:  \n",
        "                if token in self.word_dict:\n",
        "                    pass\n",
        "                else:\n",
        "                    self.word_dict[token] = value\n",
        "                    value += 1\n",
        "        self.fit_checker = True\n",
        "    \n",
        "    def transform(self, sequences):\n",
        "        result = []\n",
        "        tokens = self.preprocessing(st)\n",
        "        if self.fit_checker:\n",
        "            for multiple_token in tokens:\n",
        "                index = []\n",
        "                for token in multiple_token:\n",
        "                    index.append(self.word_dict.get(token, self.word_dict['oov']))\n",
        "                result.append(index)\n",
        "            return result\n",
        "        else:\n",
        "            raise Exception(\"Tokenizer instance is not fitted yet.\")\n",
        "    \n",
        "    def fit_transform(self, sequences):\n",
        "        self.fit(sequences)\n",
        "        result = self.transform(sequences)\n",
        "        return result"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "st = ['I Am veRY STRONG MAN^^', 'I Wanted CORE programer!!']\n",
        "tokenizer = Tokenizer()\n",
        "print(tokenizer.preprocessing(st))\n",
        "print(tokenizer.fit_transform(st))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxsicvY0rvHE",
        "outputId": "6a2f3c4d-8fb5-4e7f-af18-d7caa695c905"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['i', 'am', 'very', 'strong', 'man'], ['i', 'wanted', 'core', 'programer']]\n",
            "[[1, 2, 3, 4, 5], [1, 6, 7, 8]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from math import log\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "TM5czWU4rwoL"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TfidVectorizer:\n",
        "    def __init__(self, tokenizer):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.fit_checker = False\n",
        "    \n",
        "    def fit(self, sequences):\n",
        "        from math import log\n",
        "        tokenized = self.tokenizer.fit_transform(sequences)\n",
        "        vocab = list(set(j for i in tokenized for j in i))\n",
        "        N = len(tokenized)\n",
        "        idf = []\n",
        "        for token in vocab:\n",
        "            df = 0\n",
        "            for tokens in tokenized:\n",
        "                if token in tokens:\n",
        "                    df += 1\n",
        "            idf.append(log(N/(df+1)))\n",
        "        self.fit_checker = True\n",
        "        return idf\n",
        "    \n",
        "    def transform(self, sequences):\n",
        "        from math import log\n",
        "        if self.fit_checker:\n",
        "            tokenized = self.tokenizer.transform(sequences)\n",
        "            vocab = list(set(j for i in tokenized for j in i))\n",
        "            N = len(tokenized)\n",
        "            self.tfdf_matrix = []\n",
        "            def idf(t):\n",
        "                df = 0\n",
        "                for tokens in tokenized:\n",
        "                    if t in tokens:\n",
        "                        df += 1\n",
        "                return log(N/(df+1))\n",
        "            for i in range(N):\n",
        "                self.tfdf_matrix.append([])\n",
        "                d = tokenized[i]\n",
        "                for j in range(len(vocab)):\n",
        "                    t = vocab[j]\n",
        "                    self.tfdf_matrix[-1].append(d.count(t)*idf(t))\n",
        "            return self.tfdf_matrix\n",
        "        else:\n",
        "            raise Exception(\"TfidfVectorizer instance is not fitted yet.\")\n",
        "            \n",
        "    def fit_transform(self, sequences):\n",
        "        self.fit(sequences)\n",
        "        return self.transform(sequences)"
      ],
      "metadata": {
        "id": "xn8zcXQ3wAX3"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "tf = TfidVectorizer(tokenizer)\n",
        "print(tf.fit(st))\n",
        "print(tf.transform(st))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_7xKh9KhwDw_",
        "outputId": "3790f612-5fb5-43a3-8ce1-dfdb33bfbe29"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.40546510810816444, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "[[-0.40546510810816444, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [-0.40546510810816444, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "yisO728IwG6J"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}